{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning notes about Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understandings of the parameters in Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISILON = 0.1  # greedy degree\n",
    "LEARNING_RATE = 0.1  # learning rate\n",
    "LAMDA = 0.9  # discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EPISILON: Greedy degree. This parameter balances the exploration and exploitation. EPISILON-greedy means that when agent chooses next action, it will choose the maximum value with 1-EPISILON probability, and with EPISILON probability to choose next action randomly. (That's why Q-Learning is an off-policy algorithm. On-policy algorithm will only consider exploitation and follow the existing trategy.) At the beginning, random explorations are more useful than definite strategies, so it should not be too greedy at first, and the greedy degree should increase as the learning progress. However, in this realization, we just set a fixed greedy degree as 0.1 to simplify.\n",
    "---\n",
    "> LAMDA: Discount factor. This parameter deals with the trade-off in immediate and long-term benefits. LAMDA=1 means that the agent will take all future cumulative reward into acount when update strategy, while LAMDA=0 means the agent will only consider the immediate reward get from current action. The bigger the LAMDA is, the agent will be more visionary.(This vision is based on experience/memory, i.e. current q-table value.）\n",
    "---\n",
    "> LEARNING_RATE: Learning rate. This parameter is used to weigh the retention of previous training results. It presents that how much of the error between estimated value(previous result) and actual value(current&future rewards related) is to be learned in each step. The bigger the learning rate is, the less previous training results will be preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudocode\n",
    "def choose_action(current_S,q_table):\n",
    "    if random_num>1-EPISILON: # not greedy\n",
    "        action_name=random_choice_from(ACTION_SPACE)\n",
    "    else: # greedy\n",
    "        action_name=q_table.iloc[current_S, :].idxmax()\n",
    "    return action_name\n",
    "\n",
    "def step(current_S,choosen_A):\n",
    "    next_S,R = transform_from(env, current_S, choosen_A)\n",
    "    return next_S,R\n",
    "\n",
    "is_terminal = False\n",
    "initialize(S, q_table)\n",
    "while not is_terminal:\n",
    "    A=choose_action(S,q_table)\n",
    "    S_,R=step(S,A)\n",
    "    # estimated Q(s,a) value\n",
    "    q_predict=q_table.loc[S,A] \n",
    "    # It's similar to the recursive in dynamic programming problem（remember the recursive table?)\n",
    "    if S_!=\"terminal\":\n",
    "        # actual Q(s,a) value\n",
    "        q_target = R+LAMDA*q_table.iloc[S_,:].max()\n",
    "    else:\n",
    "        q_target = R\n",
    "        is_terminal = True\n",
    "    # update q_table\n",
    "    table.loc[S,A] += LEARNING_RATE*(q_target-q_predict)\n",
    "    S = S_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}